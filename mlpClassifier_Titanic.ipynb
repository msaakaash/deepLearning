{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from tensorflow import keras\n",
        "import tensorflow"
      ],
      "metadata": {
        "id": "Ocj5MYWLK7CD"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "jW83B54DFnPi"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare']\n",
        "target = 'Survived'"
      ],
      "metadata": {
        "id": "OKOx1c-fLNvw"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[features + [target]].dropna()"
      ],
      "metadata": {
        "id": "Mv16X7ocLaQv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode categorical features\n",
        "label_encoder = LabelEncoder()\n",
        "df['Sex'] = label_encoder.fit_transform(df['Sex'])"
      ],
      "metadata": {
        "id": "JlsfOPJiLL7W"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data into train and test\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "xtrain, xtest, ytrain, ytest = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "pTchNfd7Lby_"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Optimizers"
      ],
      "metadata": {
        "id": "sUDKsbhOQ1Uh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "optimizers = {\n",
        "    \"SGD\": keras.optimizers.SGD(learning_rate=0.01),\n",
        "    \"NAG (Nesterov)\": keras.optimizers.SGD(learning_rate=0.01, momentum=0.9, nesterov=True),\n",
        "    \"Adam\": keras.optimizers.Adam(learning_rate=0.001),\n",
        "    \"RMSprop\": keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "37D5NNe_LwZb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape target variable\n",
        "ytrain = ytrain.values.reshape(-1, 1)\n",
        "ytest = ytest.values.reshape(-1, 1)\n"
      ],
      "metadata": {
        "id": "Opve6MG-Rauy"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Creating and train model with different optimizers"
      ],
      "metadata": {
        "id": "MwGuhqS_RFTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow.config.run_functions_eagerly(True)"
      ],
      "metadata": {
        "id": "ksloWHvRSRWy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = xtrain.shape[1]\n",
        "results = {}\n",
        "for name, opt in optimizers.items():\n",
        "    model = keras.Sequential([\n",
        "        keras.layers.Input(shape=(input_shape,)),\n",
        "        keras.layers.Dense(100, activation='relu'),\n",
        "        keras.layers.Dense(50, activation='relu'),\n",
        "        keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    model.fit(xtrain, ytrain, epochs=50, batch_size=32, verbose=0)\n",
        "    _, accuracy = model.evaluate(xtest, ytest, verbose=0)\n",
        "    results[name] = accuracy\n",
        "    print(f'Optimizer: {name}, Accuracy: {accuracy:.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SJVHeSR8NbPv",
        "outputId": "e011c60b-5e06-49e5-ffea-a25384b2b88f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/tensorflow/python/data/ops/structured_function.py:258: UserWarning: Even though the `tf.config.experimental_run_functions_eagerly` option is set, this option does not apply to tf.data functions. To force eager execution of tf.data functions, please use `tf.data.experimental.enable_debug_mode()`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Unknown variable: <Variable path=sequential_4/dense_12/kernel, shape=(6, 100), dtype=float32, value=[[ 0.11241059  0.18933539 -0.23145615 -0.08541028 -0.18196514 -0.08306091\n  -0.17179015 -0.18163592 -0.20834638  0.19034348 -0.12979355 -0.11269008\n   0.18716033 -0.10832237 -0.08733678  0.09369008  0.04595555 -0.16532719\n   0.00767094 -0.06432265  0.11555682  0.00376002 -0.15573138  0.0435047\n  -0.15348446  0.1394061   0.20205007 -0.10013002  0.15463613 -0.02817574\n  -0.23312251  0.04018559 -0.02953455 -0.17366531  0.22275896 -0.1769428\n   0.00600049  0.05021049 -0.22706279  0.19407974 -0.20509471 -0.06177986\n  -0.04500899 -0.15240052 -0.0269437  -0.1769964  -0.22605613 -0.10688686\n  -0.05056648  0.14690624  0.23144837 -0.0452524   0.06514998  0.11815386\n  -0.03607346  0.12335272 -0.1951909   0.14861958  0.03619649  0.09868439\n  -0.00148162 -0.1807284   0.10838245 -0.10838896 -0.00558482  0.13849778\n  -0.0922441  -0.01684648  0.22661383 -0.19561037 -0.05340226  0.20010479\n   0.1379125   0.0783722  -0.1782366   0.02101441 -0.09974022 -0.20172487\n  -0.06841321 -0.16224284  0.08748268 -0.03160155 -0.10183354  0.06800808\n   0.08826829 -0.04732558 -0.08796057  0.20412298  0.1647958   0.17119749\n   0.00598036  0.05160283  0.10544138  0.12313889 -0.12319317  0.17417042\n  -0.05753592 -0.02552386  0.04609235 -0.08426011]\n [ 0.14379512  0.12584616 -0.06123315  0.01818101 -0.16220996  0.00586799\n   0.04077761 -0.03673786  0.06183748  0.10295825 -0.22792159 -0.21559444\n   0.11646609  0.07857816 -0.18336247 -0.0174301   0.20687236  0.15232237\n  -0.2108919  -0.11047423 -0.08197488  0.2148069  -0.14430654  0.08541544\n   0.20136066  0.23363228 -0.01227859  0.15535118  0.10110448 -0.08917435\n  -0.02242948  0.21358003  0.07716684  0.16740842  0.1743358   0.15578993\n   0.05676086 -0.00336812  0.05960934  0.13067834  0.01865084 -0.09325168\n  -0.10016434  0.19066198  0.10699691 -0.10374109  0.21227072  0.21199681\n  -0.14644797  0.1102901   0.19619076  0.03328578 -0.22346126 -0.11451685\n  -0.09303273 -0.12117682  0.04582076  0.10683133 -0.05990906  0.03070168\n   0.15764762  0.22118653 -0.12064197  0.15134321  0.15089224  0.11095156\n  -0.08376712 -0.1214555  -0.08046275  0.13449718 -0.0522645   0.17557643\n   0.08225958 -0.06264541 -0.13321649 -0.14476663 -0.13150576  0.03388105\n  -0.17672123 -0.01528113  0.1928768   0.10742615 -0.03067084 -0.0590371\n  -0.01364665  0.0290731   0.01764865  0.03386174 -0.21791188 -0.08318616\n  -0.20543629 -0.17840949 -0.1976021  -0.20145215  0.137488   -0.0385035\n  -0.10684749  0.06181695  0.09904774 -0.15285335]\n [-0.00638758  0.1437131  -0.20940132  0.14707352  0.06841265  0.05975504\n   0.09975486  0.19198199 -0.23281966 -0.1173621   0.09135814  0.02023433\n  -0.06373262 -0.1148167  -0.10813978 -0.11054508  0.18033634  0.19437815\n   0.09955512 -0.0527387   0.21349232  0.18448178 -0.0999973   0.20156379\n   0.2117262  -0.11081366 -0.15720688 -0.12372149  0.06297718 -0.17510927\n  -0.09902506  0.0919915   0.12625687  0.20252793  0.05869199  0.04664598\n  -0.03136779 -0.01712266  0.02711602 -0.07254121 -0.07166296 -0.16528356\n   0.23009978  0.00259085  0.15419696 -0.02673161 -0.13000298 -0.08234341\n  -0.01005463  0.09733452  0.09974645 -0.20387061 -0.15690964 -0.09948832\n   0.00668509 -0.18113278  0.12681775 -0.15638122  0.08618967 -0.02573441\n   0.17629711 -0.07788189 -0.00775342 -0.20742717 -0.07519796 -0.11677065\n  -0.11458867 -0.14030108  0.09490941  0.19677518 -0.0166923  -0.06883161\n   0.16276346 -0.03761804  0.02204274  0.11798419  0.1849774   0.15017639\n   0.12520666 -0.06398475 -0.08450407 -0.12492414 -0.01462075 -0.04883334\n   0.09143476 -0.11152644  0.02484091 -0.09140345 -0.17980364 -0.11720129\n   0.22861771  0.14673708 -0.18152377  0.07461251 -0.04625112  0.21525325\n   0.02139588 -0.06294331 -0.17001045 -0.17533241]\n [ 0.1999609  -0.02774441  0.15161963 -0.15959466  0.05299862 -0.01066509\n   0.08844732  0.1611145  -0.00976512 -0.12469208 -0.10453647 -0.22739163\n   0.21094294 -0.20022063 -0.02855159  0.23371239 -0.21624722  0.20913257\n   0.06545974 -0.00497942  0.06177096  0.22873114  0.20469226  0.02703707\n   0.1522003   0.00667624 -0.04817858 -0.18480489  0.14337425 -0.15277779\n   0.07266848 -0.07475847  0.05608727 -0.22568794  0.1371844  -0.18196326\n  -0.1567882   0.00434995  0.22966589 -0.134107    0.18167551  0.04744335\n   0.18383951  0.00031193  0.15620492 -0.12359499 -0.18677178  0.07437928\n  -0.23431592  0.18500309  0.10298951  0.10722943 -0.09552583 -0.01613924\n   0.14333324 -0.19444549 -0.0386567   0.19284539  0.08855025  0.16338323\n   0.09870769  0.15997095  0.18275763  0.19088791  0.17369531 -0.15018657\n   0.13339256 -0.08790816 -0.02753851 -0.04842058 -0.02184318 -0.1817518\n   0.19642203 -0.23584229  0.18720992  0.09997807  0.04751311  0.21319391\n   0.14933123  0.16280459 -0.0176035  -0.10608922  0.00222997 -0.21976607\n  -0.05624183 -0.22046365 -0.0804112   0.1914476  -0.2368966   0.1204942\n  -0.20915288 -0.1369966  -0.18708892  0.1424215   0.10968305 -0.08891256\n   0.02905621 -0.13327605  0.16774674 -0.1899049 ]\n [-0.0800574   0.2321317  -0.23591943  0.18599267  0.01141538  0.20426638\n   0.06903009 -0.22702587 -0.15171969 -0.2218098  -0.21308477  0.00364375\n   0.19236986  0.2170469   0.13111387  0.1698785   0.15602528 -0.17681828\n   0.17017467 -0.17152303  0.20215382 -0.06045894  0.05132999 -0.03354847\n   0.16490908 -0.23320703  0.2236575  -0.00676008 -0.10547951 -0.04121527\n  -0.0311261  -0.01669399  0.01263918 -0.22867697  0.17588903 -0.13664065\n   0.16312177 -0.18499628 -0.11491749 -0.10262938 -0.15971044 -0.15516052\n  -0.08963636  0.05376591  0.0239356   0.14340015 -0.18214065  0.03061791\n   0.09309585 -0.11148374 -0.0692884   0.02081253 -0.12216897  0.16221415\n  -0.18809922  0.02355634  0.11839901  0.05346091 -0.05088221 -0.14253558\n  -0.0767438   0.05132352  0.11996503  0.1086394  -0.14752492  0.14422776\n  -0.07353257 -0.01299256 -0.07747479  0.02170043  0.22131886 -0.14454508\n  -0.14037097  0.14686869 -0.22237206 -0.21434075 -0.23665275 -0.20617568\n   0.20463155 -0.15341061 -0.23364499  0.10799508  0.03844644 -0.14255947\n  -0.10754861 -0.17451923 -0.15152597  0.04163237 -0.13228554  0.01677807\n   0.21576647  0.02126847  0.08622672 -0.1403224   0.1394314  -0.02666321\n   0.00081824 -0.11579654  0.23772804  0.04508193]\n [ 0.08130015  0.22397165  0.10685174  0.10436915 -0.02751666  0.090342\n   0.09200139  0.20129167 -0.20352097 -0.14098726 -0.04991132 -0.10393719\n   0.14511032  0.1998622  -0.23028833 -0.20602275  0.00206853 -0.1201204\n   0.01358585  0.06609856 -0.17949598 -0.06306027 -0.10284816 -0.18667677\n  -0.16927072  0.16479774 -0.10400799  0.00170687  0.12550117 -0.02723669\n  -0.05811808  0.17493542 -0.07337436 -0.11806496  0.12476997 -0.21991508\n  -0.22528668 -0.21673419  0.11567073  0.01669042 -0.22782551 -0.23145688\n   0.18486644  0.18515114 -0.01478747  0.00485598 -0.19011648 -0.21353027\n  -0.17857303 -0.05026215 -0.1381263   0.08689831 -0.15454836 -0.0941959\n  -0.04208359 -0.0113477  -0.23474003 -0.18837024  0.16779624 -0.09700757\n   0.09147017  0.09100719  0.06637304 -0.07735488  0.005477    0.06163962\n  -0.16216134 -0.07935613 -0.09674738  0.07679115 -0.13009754 -0.01130544\n   0.16445403 -0.00917087 -0.04493316  0.04195653  0.09723894  0.09591667\n  -0.09699605 -0.23357527  0.00034873 -0.011308    0.1476974   0.16790284\n  -0.00078823  0.00036179 -0.22293077 -0.22292465  0.03541417  0.21757714\n   0.1598797  -0.02415994  0.11925407  0.12455057  0.07372616  0.07110588\n  -0.06162466  0.03063522  0.21872519  0.1651055 ]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-a8f6f8a4a25e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     ])\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mytest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py\u001b[0m in \u001b[0;36m_check_variables_are_known\u001b[0;34m(self, variables)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_var_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_variables_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 raise ValueError(\n\u001b[0m\u001b[1;32m    330\u001b[0m                     \u001b[0;34mf\"Unknown variable: {v}. This optimizer can only \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                     \u001b[0;34m\"be called for the variables it was originally built with. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Unknown variable: <Variable path=sequential_4/dense_12/kernel, shape=(6, 100), dtype=float32, value=[[ 0.11241059  0.18933539 -0.23145615 -0.08541028 -0.18196514 -0.08306091\n  -0.17179015 -0.18163592 -0.20834638  0.19034348 -0.12979355 -0.11269008\n   0.18716033 -0.10832237 -0.08733678  0.09369008  0.04595555 -0.16532719\n   0.00767094 -0.06432265  0.11555682  0.00376002 -0.15573138  0.0435047\n  -0.15348446  0.1394061   0.20205007 -0.10013002  0.15463613 -0.02817574\n  -0.23312251  0.04018559 -0.02953455 -0.17366531  0.22275896 -0.1769428\n   0.00600049  0.05021049 -0.22706279  0.19407974 -0.20509471 -0.06177986\n  -0.04500899 -0.15240052 -0.0269437  -0.1769964  -0.22605613 -0.10688686\n  -0.05056648  0.14690624  0.23144837 -0.0452524   0.06514998  0.11815386\n  -0.03607346  0.12335272 -0.1951909   0.14861958  0.03619649  0.09868439\n  -0.00148162 -0.1807284   0.10838245 -0.10838896 -0.00558482  0.13849778\n  -0.0922441  -0.01684648  0.22661383 -0.19561037 -0.05340226  0.20010479\n   0.1379125   0.0783722  -0.1782366   0.02101441 -0.09974022 -0.20172487\n  -0.06841321 -0.16224284  0.08748268 -0.03160155 -0.10183354  0.06800808\n   0.08826829 -0.04732558 -0.08796057  0.20412298  0.1647958   0.17119749\n   0.00598036  0.05160283  0.10544138  0.12313889 -0.12319317  0.17417042\n  -0.05753592 -0.02552386  0.04609235 -0.08426011]\n [ 0.14379512  0.12584616 -0.06123315  0.01818101 -0.16220996  0.00586799\n   0.04077761 -0.03673786  0.06183748  0.10295825 -0.22792159 -0.21559444\n   0.11646609  0.07857816 -0.18336247 -0.0174301   0.20687236  0.15232237\n  -0.2108919  -0.11047423 -0.08197488  0.2148069  -0.14430654  0.08541544\n   0.20136066  0.23363228 -0.01227859  0.15535118  0.10110448 -0.08917435\n  -0.02242948  0.21358003  0.07716684  0.16740842  0.1743358   0.15578993\n   0.05676086 -0.00336812  0.05960934  0.13067834  0.01865084 -0.09325168\n  -0.10016434  0.19066198  0.10699691 -0.10374109  0.21227072  0.21199681\n  -0.14644797  0.1102901   0.19619076  0.03328578 -0.22346126 -0.11451685\n  -0.09303273 -0.12117682  0.04582076  0.10683133 -0.05990906  0.03070168\n   0.15764762  0.22118653 -0.12064197  0.15134321  0.15089224  0.11095156\n  -0.08376712 -0.1214555  -0.08046275  0.13449718 -0.0522645   0.17557643\n   0.08225958 -0.06264541 -0.13321649 -0.14476663 -0.13150576  0.03388105\n  -0.17672123 -0.01528113  0.1928768   0.10742615 -0.03067084 -0.0590371\n  -0.01364665  0.0290731   0.01764865  0.03386174 -0.21791188 -0.08318616\n  -0.20543629 -0.17840949 -0.1976021  -0.20145215  0.137488   -0.0385035\n  -0.10684749  0.06181695  0.09904774 -0.15285335]\n [-0.00638758  0.1437131  -0.20940132  0.14707352  0.06841265  0.05975504\n   0.09975486  0.19198199 -0.23281966 -0.1173621   0.09135814  0.02023433\n  -0.06373262 -0.1148167  -0.10813978 -0.11054508  0.18033634  0.19437815\n   0.09955512 -0.0527387   0.21349232  0.18448178 -0.0999973   0.20156379\n   0.2117262  -0.11081366 -0.15720688 -0.12372149  0.06297718 -0.17510927\n  -0.09902506  0.0919915   0.12625687  0.20252793  0.05869199  0.04664598\n  -0.03136779 -0.01712266  0.02711602 -0.07254121 -0.07166296 -0.16528356\n   0.23009978  0.00259085  0.15419696 -0.02673161 -0.13000298 -0.08234341\n  -0.01005463  0.09733452  0.09974645 -0.20387061 -0.15690964 -0.09948832\n   0.00668509 -0.18113278  0.12681775 -0.15638122  0.08618967 -0.02573441\n   0.17629711 -0.07788189 -0.00775342 -0.20742717 -0.07519796 -0.11677065\n  -0.11458867 -0.14030108  0.09490941  0.19677518 -0.0166923  -0.06883161\n   0.16276346 -0.03761804  0.02204274  0.11798419  0.1849774   0.15017639\n   0.12520666 -0.06398475 -0.08450407 -0.12492414 -0.01462075 -0.04883334\n   0.09143476 -0.11152644  0.02484091 -0.09140345 -0.17980364 -0.11720129\n   0.22861771  0.14673708 -0.18152377  0.07461251 -0.04625112  0.21525325\n   0.02139588 -0.06294331 -0.17001045 -0.17533241]\n [ 0.1999609  -0.02774441  0.15161963 -0.15959466  0.05299862 -0.01066509\n   0.08844732  0.1611145  -0.00976512 -0.12469208 -0.10453647 -0.22739163\n   0.21094294 -0.20022063 -0.02855159  0.23371239 -0.21624722  0.20913257\n   0.06545974 -0.00497942  0.06177096  0.22873114  0.20469226  0.02703707\n   0.1522003   0.00667624 -0.04817858 -0.18480489  0.14337425 -0.15277779\n   0.07266848 -0.07475847  0.05608727 -0.22568794  0.1371844  -0.18196326\n  -0.1567882   0.00434995  0.22966589 -0.134107    0.18167551  0.04744335\n   0.18383951  0.00031193  0.15620492 -0.12359499 -0.18677178  0.07437928\n  -0.23431592  0.18500309  0.10298951  0.10722943 -0.09552583 -0.01613924\n   0.14333324 -0.19444549 -0.0386567   0.19284539  0.08855025  0.16338323\n   0.09870769  0.15997095  0.18275763  0.19088791  0.17369531 -0.15018657\n   0.13339256 -0.08790816 -0.02753851 -0.04842058 -0.02184318 -0.1817518\n   0.19642203 -0.23584229  0.18720992  0.09997807  0.04751311  0.21319391\n   0.14933123  0.16280459 -0.0176035  -0.10608922  0.00222997 -0.21976607\n  -0.05624183 -0.22046365 -0.0804112   0.1914476  -0.2368966   0.1204942\n  -0.20915288 -0.1369966  -0.18708892  0.1424215   0.10968305 -0.08891256\n   0.02905621 -0.13327605  0.16774674 -0.1899049 ]\n [-0.0800574   0.2321317  -0.23591943  0.18599267  0.01141538  0.20426638\n   0.06903009 -0.22702587 -0.15171969 -0.2218098  -0.21308477  0.00364375\n   0.19236986  0.2170469   0.13111387  0.1698785   0.15602528 -0.17681828\n   0.17017467 -0.17152303  0.20215382 -0.06045894  0.05132999 -0.03354847\n   0.16490908 -0.23320703  0.2236575  -0.00676008 -0.10547951 -0.04121527\n  -0.0311261  -0.01669399  0.01263918 -0.22867697  0.17588903 -0.13664065\n   0.16312177 -0.18499628 -0.11491749 -0.10262938 -0.15971044 -0.15516052\n  -0.08963636  0.05376591  0.0239356   0.14340015 -0.18214065  0.03061791\n   0.09309585 -0.11148374 -0.0692884   0.02081253 -0.12216897  0.16221415\n  -0.18809922  0.02355634  0.11839901  0.05346091 -0.05088221 -0.14253558\n  -0.0767438   0.05132352  0.11996503  0.1086394  -0.14752492  0.14422776\n  -0.07353257 -0.01299256 -0.07747479  0.02170043  0.22131886 -0.14454508\n  -0.14037097  0.14686869 -0.22237206 -0.21434075 -0.23665275 -0.20617568\n   0.20463155 -0.15341061 -0.23364499  0.10799508  0.03844644 -0.14255947\n  -0.10754861 -0.17451923 -0.15152597  0.04163237 -0.13228554  0.01677807\n   0.21576647  0.02126847  0.08622672 -0.1403224   0.1394314  -0.02666321\n   0.00081824 -0.11579654  0.23772804  0.04508193]\n [ 0.08130015  0.22397165  0.10685174  0.10436915 -0.02751666  0.090342\n   0.09200139  0.20129167 -0.20352097 -0.14098726 -0.04991132 -0.10393719\n   0.14511032  0.1998622  -0.23028833 -0.20602275  0.00206853 -0.1201204\n   0.01358585  0.06609856 -0.17949598 -0.06306027 -0.10284816 -0.18667677\n  -0.16927072  0.16479774 -0.10400799  0.00170687  0.12550117 -0.02723669\n  -0.05811808  0.17493542 -0.07337436 -0.11806496  0.12476997 -0.21991508\n  -0.22528668 -0.21673419  0.11567073  0.01669042 -0.22782551 -0.23145688\n   0.18486644  0.18515114 -0.01478747  0.00485598 -0.19011648 -0.21353027\n  -0.17857303 -0.05026215 -0.1381263   0.08689831 -0.15454836 -0.0941959\n  -0.04208359 -0.0113477  -0.23474003 -0.18837024  0.16779624 -0.09700757\n   0.09147017  0.09100719  0.06637304 -0.07735488  0.005477    0.06163962\n  -0.16216134 -0.07935613 -0.09674738  0.07679115 -0.13009754 -0.01130544\n   0.16445403 -0.00917087 -0.04493316  0.04195653  0.09723894  0.09591667\n  -0.09699605 -0.23357527  0.00034873 -0.011308    0.1476974   0.16790284\n  -0.00078823  0.00036179 -0.22293077 -0.22292465  0.03541417  0.21757714\n   0.1598797  -0.02415994  0.11925407  0.12455057  0.07372616  0.07110588\n  -0.06162466  0.03063522  0.21872519  0.1651055 ]]>. This optimizer can only be called for the variables it was originally built with. When working with a new set of variables, you should recreate a new optimizer instance."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Vk_vaz4MY22"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}