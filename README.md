# 🧠 DEEP LEARNING  

## 📌 Overview  
This repository is dedicated to deep learning concepts, covering activation functions, gradient descent techniques, and multi-layer perceptron (MLP) classifiers for various datasets. It serves as a great starting point for understanding and implementing deep learning models.  

## 📂 Repository Structure  

- **Gradient Descent** – Implementation of various variants of Gradient Descent, including Stochastic Gradient Descent, Batch Gradient Descent, and more.
- **Loss** - Implementation of various loss functions.
- **Multi-Layer Perceptron** – Implementation of MLP classifiers using various datasets for training and evaluation.
- **activation_functions.ipynb** -  Implementation and analysis of various activation functions used in neural networks.

## 🚀 Getting Started  

### Prerequisites  
To run the notebooks, ensure you have the following installed:  
- Python (>=3.7)  
- Jupyter Notebook  
- TensorFlow / PyTorch  
- NumPy, Pandas, Matplotlib  

### Installation  
Clone the repository:  
```bash
git clone https://github.com/msaakaash/deepLearning.git
cd deepLearning
```
Run Jupyter Notebook:  
```bash
jupyter notebook
```

## 📊 Datasets  
- **MNIST**: A dataset for handwritten digit classification.  
- **CIFAR**: A popular dataset for image classification tasks.  
- **Iris**: A classic dataset used for classification tasks.  
- **Bangalore Home Prices**: A dataset for real estate price prediction.  

## 🔥 Key Topics Covered  
✔ Activation Functions in Neural Networks  
✔ Gradient Descent Variants  
✔ Multi-Layer Perceptron (MLP) Models  
✔ Image Classification with Neural Networks  

## 🛠️ Future Improvements  
- Implement Convolutional Neural Networks (CNNs).  
- Explore additional optimization techniques.  
- Add more advanced deep learning models.  

## 🤝 Contributing  
Contributions are welcome! Feel free to fork the repository, work on new features, and submit pull requests.  

## 📝 License  
This project is licensed under the MIT License.  

